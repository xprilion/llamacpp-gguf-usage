{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xprilion/llamacpp-gguf-usage/blob/main/Gemma3GGUF_LLamaCPP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irc-vi4pQxXM",
        "outputId": "67328070-a143-4c76-9792-71d2a701fe33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m262.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m301.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m272.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m355.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m277.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m255.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=53369831 sha256=bf6a5dc7fb8fef64fd84898f4a96677016afe3aeaf5489cc15fd0dd8f69c5df3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_y4l04of/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.3.8\n",
            "    Uninstalling llama_cpp_python-0.3.8:\n",
            "      Successfully uninstalled llama_cpp_python-0.3.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.8 numpy-2.2.5 typing-extensions-4.13.2\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mF9wach2QdlY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and pull ollama model files\n",
        "\n",
        "1. Download and install Ollama into the system\n",
        "2. Run the Ollama serve in a separate terminal if you are in a restricted environment where the server is not able to auto start\n",
        "3. Then run the command below to pull model"
      ],
      "metadata": {
        "id": "8gFhLZOGkOkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK3TJPnTbPix",
        "outputId": "c7fdc65a-574a-47bf-dd2d-59cc668cfb48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_path = \"/root/.ollama/models/blobs/sha256-7cd4618c1faf8b7233c6c906dac1694b6a47684b37b8895d470ac688520b9c01\"\n",
        "\n",
        "print(\"My model path: \", model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk_LR5ZSjHgO",
        "outputId": "c1eb7ff0-98c3-4d2b-d6a7-a77fe964b598"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My model path:  /root/.ollama/models/blobs/sha256-7cd4618c1faf8b7233c6c906dac1694b6a47684b37b8895d470ac688520b9c01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70_mXTOZUQhN",
        "outputId": "fe66a4fb-2522-47a8-fdfc-8d1d3f691e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 340 tensors from /root/.ollama/models/blobs/sha256-7cd4618c1faf8b7233c6c906dac1694b6a47684b37b8895d470ac688520b9c01 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                gemma3.attention.head_count u32              = 4\n",
            "llama_model_loader: - kv   1:             gemma3.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   2:                gemma3.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   3:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv   4:            gemma3.attention.sliding_window u32              = 512\n",
            "llama_model_loader: - kv   5:              gemma3.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv   6:                         gemma3.block_count u32              = 26\n",
            "llama_model_loader: - kv   7:                      gemma3.context_length u32              = 32768\n",
            "llama_model_loader: - kv   8:                    gemma3.embedding_length u32              = 1152\n",
            "llama_model_loader: - kv   9:                 gemma3.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv  10:             gemma3.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  11:               gemma3.rope.global.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  12:                gemma3.rope.local.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  13:                       general.architecture str              = gemma3\n",
            "llama_model_loader: - kv  14:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\n",
            "llama_model_loader: - kv  15:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  16:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  17:           tokenizer.ggml.add_padding_token bool             = false\n",
            "llama_model_loader: - kv  18:           tokenizer.ggml.add_unknown_token bool             = false\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,514906]  = [\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\", ...\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  157 tensors\n",
            "llama_model_loader: - type q5_0:  117 tensors\n",
            "llama_model_loader: - type q8_0:   14 tensors\n",
            "llama_model_loader: - type q4_K:   39 tensors\n",
            "llama_model_loader: - type q6_K:   13 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 762.49 MiB (6.40 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:    105 '<start_of_turn>' is not marked as EOG\n",
            "load: control token:      2 '<bos>' is not marked as EOG\n",
            "load: control token:      0 '<pad>' is not marked as EOG\n",
            "load: control token:      1 '<eos>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 6\n",
            "load: token to piece cache size = 1.9446 MB\n",
            "print_info: arch             = gemma3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1152\n",
            "print_info: n_layer          = 26\n",
            "print_info: n_head           = 4\n",
            "print_info: n_head_kv        = 1\n",
            "print_info: n_rot            = 256\n",
            "print_info: n_swa            = 512\n",
            "print_info: n_embd_head_k    = 256\n",
            "print_info: n_embd_head_v    = 256\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 6.2e-02\n",
            "print_info: n_ff             = 6912\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1B\n",
            "print_info: model params     = 999.89 M\n",
            "print_info: general.name     = n/a\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 262144\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 2 '<bos>'\n",
            "print_info: EOS token        = 1 '<eos>'\n",
            "print_info: EOT token        = 106 '<end_of_turn>'\n",
            "print_info: UNK token        = 3 '<unk>'\n",
            "print_info: PAD token        = 0 '<pad>'\n",
            "print_info: LF token         = 248 '<0x0A>'\n",
            "print_info: EOG token        = 1 '<eos>'\n",
            "print_info: EOG token        = 106 '<end_of_turn>'\n",
            "print_info: max token length = 93\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0\n",
            "load_tensors: layer   1 assigned to device CUDA0\n",
            "load_tensors: layer   2 assigned to device CUDA0\n",
            "load_tensors: layer   3 assigned to device CUDA0\n",
            "load_tensors: layer   4 assigned to device CUDA0\n",
            "load_tensors: layer   5 assigned to device CUDA0\n",
            "load_tensors: layer   6 assigned to device CUDA0\n",
            "load_tensors: layer   7 assigned to device CUDA0\n",
            "load_tensors: layer   8 assigned to device CUDA0\n",
            "load_tensors: layer   9 assigned to device CUDA0\n",
            "load_tensors: layer  10 assigned to device CUDA0\n",
            "load_tensors: layer  11 assigned to device CUDA0\n",
            "load_tensors: layer  12 assigned to device CUDA0\n",
            "load_tensors: layer  13 assigned to device CUDA0\n",
            "load_tensors: layer  14 assigned to device CUDA0\n",
            "load_tensors: layer  15 assigned to device CUDA0\n",
            "load_tensors: layer  16 assigned to device CUDA0\n",
            "load_tensors: layer  17 assigned to device CUDA0\n",
            "load_tensors: layer  18 assigned to device CUDA0\n",
            "load_tensors: layer  19 assigned to device CUDA0\n",
            "load_tensors: layer  20 assigned to device CUDA0\n",
            "load_tensors: layer  21 assigned to device CUDA0\n",
            "load_tensors: layer  22 assigned to device CUDA0\n",
            "load_tensors: layer  23 assigned to device CUDA0\n",
            "load_tensors: layer  24 assigned to device CUDA0\n",
            "load_tensors: layer  25 assigned to device CUDA0\n",
            "load_tensors: layer  26 assigned to device CUDA0\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors: offloading 26 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 27/27 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =   762.54 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   306.00 MiB\n",
            ".............................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 26, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    13.00 MiB\n",
            "llama_init_from_model: KV self size  =   13.00 MiB, K (f16):    6.50 MiB, V (f16):    6.50 MiB\n",
            "llama_init_from_model:  CUDA_Host  output buffer size =     1.00 MiB\n",
            "llama_init_from_model:      CUDA0 compute buffer size =   514.25 MiB\n",
            "llama_init_from_model:  CUDA_Host compute buffer size =     4.26 MiB\n",
            "llama_init_from_model: graph nodes  = 1047\n",
            "llama_init_from_model: graph splits = 2\n",
            "CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.add_unknown_token': 'false', 'tokenizer.ggml.add_padding_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'general.architecture': 'gemma3', 'tokenizer.chat_template': '{{ bos_token }}\\n{%- if messages[0][\\'role\\'] == \\'system\\' -%}\\n    {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\n\\n\\' -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \"\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\\n    {%- endif -%}\\n    {%- if (message[\\'role\\'] == \\'assistant\\') -%}\\n        {%- set role = \"model\" -%}\\n    {%- else -%}\\n        {%- set role = message[\\'role\\'] -%}\\n    {%- endif -%}\\n    {{ \\'<start_of_turn>\\' + role + \\'\\n\\' + (first_user_prefix if loop.first else \"\") }}\\n    {%- if message[\\'content\\'] is string -%}\\n        {{ message[\\'content\\'] | trim }}\\n    {%- elif message[\\'content\\'] is iterable -%}\\n        {%- for item in message[\\'content\\'] -%}\\n            {%- if item[\\'type\\'] == \\'image\\' -%}\\n                {{ \\'<start_of_image>\\' }}\\n            {%- elif item[\\'type\\'] == \\'text\\' -%}\\n                {{ item[\\'text\\'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\"Invalid content type\") }}\\n    {%- endif -%}\\n    {{ \\'<end_of_turn>\\n\\' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{\\'<start_of_turn>model\\n\\'}}\\n{%- endif -%}\\n', 'gemma3.attention.head_count': '4', 'gemma3.attention.head_count_kv': '1', 'gemma3.attention.layer_norm_rms_epsilon': '0.000001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'gemma3.attention.sliding_window': '512', 'gemma3.attention.key_length': '256', 'gemma3.embedding_length': '1152', 'gemma3.block_count': '26', 'gemma3.context_length': '32768', 'gemma3.attention.value_length': '256', 'gemma3.final_logit_softcapping': '30.000000', 'tokenizer.ggml.eos_token_id': '1', 'gemma3.feed_forward_length': '6912', 'gemma3.rope.global.freq_base': '1000000.000000', 'gemma3.rope.local.freq_base': '10000.000000'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{'<start_of_turn>model\n",
            "'}}\n",
            "{%- endif -%}\n",
            "\n",
            "Using chat eos_token: <eos>\n",
            "Using chat bos_token: <bos>\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=model_path,\n",
        "            n_gpu_layers=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=model_path,\n",
        "            n_gpu_layers=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO2R-xv3jxZF",
        "outputId": "a66d4008-ace6-42c0-9904-ba4cd0d298c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 13698 MiB free\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 340 tensors from /root/.ollama/models/blobs/sha256-7cd4618c1faf8b7233c6c906dac1694b6a47684b37b8895d470ac688520b9c01 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                gemma3.attention.head_count u32              = 4\n",
            "llama_model_loader: - kv   1:             gemma3.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   2:                gemma3.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   3:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv   4:            gemma3.attention.sliding_window u32              = 512\n",
            "llama_model_loader: - kv   5:              gemma3.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv   6:                         gemma3.block_count u32              = 26\n",
            "llama_model_loader: - kv   7:                      gemma3.context_length u32              = 32768\n",
            "llama_model_loader: - kv   8:                    gemma3.embedding_length u32              = 1152\n",
            "llama_model_loader: - kv   9:                 gemma3.feed_forward_length u32              = 6912\n",
            "llama_model_loader: - kv  10:             gemma3.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  11:               gemma3.rope.global.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  12:                gemma3.rope.local.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  13:                       general.architecture str              = gemma3\n",
            "llama_model_loader: - kv  14:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\n",
            "llama_model_loader: - kv  15:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  16:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  17:           tokenizer.ggml.add_padding_token bool             = false\n",
            "llama_model_loader: - kv  18:           tokenizer.ggml.add_unknown_token bool             = false\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,514906]  = [\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\", ...\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  30:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  157 tensors\n",
            "llama_model_loader: - type q5_0:  117 tensors\n",
            "llama_model_loader: - type q8_0:   14 tensors\n",
            "llama_model_loader: - type q4_K:   39 tensors\n",
            "llama_model_loader: - type q6_K:   13 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 762.49 MiB (6.40 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:    105 '<start_of_turn>' is not marked as EOG\n",
            "load: control token:      2 '<bos>' is not marked as EOG\n",
            "load: control token:      0 '<pad>' is not marked as EOG\n",
            "load: control token:      1 '<eos>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 6\n",
            "load: token to piece cache size = 1.9446 MB\n",
            "print_info: arch             = gemma3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1152\n",
            "print_info: n_layer          = 26\n",
            "print_info: n_head           = 4\n",
            "print_info: n_head_kv        = 1\n",
            "print_info: n_rot            = 256\n",
            "print_info: n_swa            = 512\n",
            "print_info: n_embd_head_k    = 256\n",
            "print_info: n_embd_head_v    = 256\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 6.2e-02\n",
            "print_info: n_ff             = 6912\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1B\n",
            "print_info: model params     = 999.89 M\n",
            "print_info: general.name     = n/a\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 262144\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 2 '<bos>'\n",
            "print_info: EOS token        = 1 '<eos>'\n",
            "print_info: EOT token        = 106 '<end_of_turn>'\n",
            "print_info: UNK token        = 3 '<unk>'\n",
            "print_info: PAD token        = 0 '<pad>'\n",
            "print_info: LF token         = 248 '<0x0A>'\n",
            "print_info: EOG token        = 1 '<eos>'\n",
            "print_info: EOG token        = 106 '<end_of_turn>'\n",
            "print_info: max token length = 93\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0\n",
            "load_tensors: layer   1 assigned to device CUDA0\n",
            "load_tensors: layer   2 assigned to device CUDA0\n",
            "load_tensors: layer   3 assigned to device CUDA0\n",
            "load_tensors: layer   4 assigned to device CUDA0\n",
            "load_tensors: layer   5 assigned to device CUDA0\n",
            "load_tensors: layer   6 assigned to device CUDA0\n",
            "load_tensors: layer   7 assigned to device CUDA0\n",
            "load_tensors: layer   8 assigned to device CUDA0\n",
            "load_tensors: layer   9 assigned to device CUDA0\n",
            "load_tensors: layer  10 assigned to device CUDA0\n",
            "load_tensors: layer  11 assigned to device CUDA0\n",
            "load_tensors: layer  12 assigned to device CUDA0\n",
            "load_tensors: layer  13 assigned to device CUDA0\n",
            "load_tensors: layer  14 assigned to device CUDA0\n",
            "load_tensors: layer  15 assigned to device CUDA0\n",
            "load_tensors: layer  16 assigned to device CUDA0\n",
            "load_tensors: layer  17 assigned to device CUDA0\n",
            "load_tensors: layer  18 assigned to device CUDA0\n",
            "load_tensors: layer  19 assigned to device CUDA0\n",
            "load_tensors: layer  20 assigned to device CUDA0\n",
            "load_tensors: layer  21 assigned to device CUDA0\n",
            "load_tensors: layer  22 assigned to device CUDA0\n",
            "load_tensors: layer  23 assigned to device CUDA0\n",
            "load_tensors: layer  24 assigned to device CUDA0\n",
            "load_tensors: layer  25 assigned to device CUDA0\n",
            "load_tensors: layer  26 assigned to device CUDA0\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors: offloading 26 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 27/27 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =   762.54 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   306.00 MiB\n",
            ".............................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 26, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    13.00 MiB\n",
            "llama_init_from_model: KV self size  =   13.00 MiB, K (f16):    6.50 MiB, V (f16):    6.50 MiB\n",
            "llama_init_from_model:  CUDA_Host  output buffer size =     1.00 MiB\n",
            "llama_init_from_model:      CUDA0 compute buffer size =   514.25 MiB\n",
            "llama_init_from_model:  CUDA_Host compute buffer size =     4.26 MiB\n",
            "llama_init_from_model: graph nodes  = 1047\n",
            "llama_init_from_model: graph splits = 2\n",
            "CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.add_unknown_token': 'false', 'tokenizer.ggml.add_padding_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'general.architecture': 'gemma3', 'tokenizer.chat_template': '{{ bos_token }}\\n{%- if messages[0][\\'role\\'] == \\'system\\' -%}\\n    {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\n\\n\\' -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \"\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\\n    {%- endif -%}\\n    {%- if (message[\\'role\\'] == \\'assistant\\') -%}\\n        {%- set role = \"model\" -%}\\n    {%- else -%}\\n        {%- set role = message[\\'role\\'] -%}\\n    {%- endif -%}\\n    {{ \\'<start_of_turn>\\' + role + \\'\\n\\' + (first_user_prefix if loop.first else \"\") }}\\n    {%- if message[\\'content\\'] is string -%}\\n        {{ message[\\'content\\'] | trim }}\\n    {%- elif message[\\'content\\'] is iterable -%}\\n        {%- for item in message[\\'content\\'] -%}\\n            {%- if item[\\'type\\'] == \\'image\\' -%}\\n                {{ \\'<start_of_image>\\' }}\\n            {%- elif item[\\'type\\'] == \\'text\\' -%}\\n                {{ item[\\'text\\'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\"Invalid content type\") }}\\n    {%- endif -%}\\n    {{ \\'<end_of_turn>\\n\\' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{\\'<start_of_turn>model\\n\\'}}\\n{%- endif -%}\\n', 'gemma3.attention.head_count': '4', 'gemma3.attention.head_count_kv': '1', 'gemma3.attention.layer_norm_rms_epsilon': '0.000001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'gemma3.attention.sliding_window': '512', 'gemma3.attention.key_length': '256', 'gemma3.embedding_length': '1152', 'gemma3.block_count': '26', 'gemma3.context_length': '32768', 'gemma3.attention.value_length': '256', 'gemma3.final_logit_softcapping': '30.000000', 'tokenizer.ggml.eos_token_id': '1', 'gemma3.feed_forward_length': '6912', 'gemma3.rope.global.freq_base': '1000000.000000', 'gemma3.rope.local.freq_base': '10000.000000'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{'<start_of_turn>model\n",
            "'}}\n",
            "{%- endif -%}\n",
            "\n",
            "Using chat eos_token: <eos>\n",
            "Using chat bos_token: <bos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(\"What is an LLM?\", max_tokens=1000)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uBK2LPUj8OQ",
        "outputId": "503feaf4-786d-4b25-a038-43faed55e81c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     138.72 ms\n",
            "llama_perf_context_print: prompt eval time =     138.53 ms /     7 tokens (   19.79 ms per token,    50.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3708.99 ms /   395 runs   (    9.39 ms per token,   106.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    5238.85 ms /   402 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-43de42f3-d382-4bfa-b90d-9c89301e7448',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1745127008,\n",
              " 'model': '/root/.ollama/models/blobs/sha256-7cd4618c1faf8b7233c6c906dac1694b6a47684b37b8895d470ac688520b9c01',\n",
              " 'choices': [{'text': '\\n**Large Language Model**\\n\\nA Large Language Model (LLM) is a type of artificial intelligence (AI) that is trained on a massive amount of text data.  These models are designed to understand and generate human-like text.  They can perform a variety of tasks, such as:\\n\\n*   **Answering questions:** You can ask them questions and they will try to provide answers.\\n*   **Generating text:** They can write stories, poems, code, or anything else you request.\\n*   **Summarizing text:** They can quickly condense lengthy articles or documents.\\n*   **Translation:** They can translate text from one language to another.\\n*   **Chatting:** They can engage in conversations.\\n\\nLLMs are a particularly powerful type of AI because of their ability to learn from vast amounts of data.\\n\\n**Examples of LLMs:**\\n\\n*   **ChatGPT:** Developed by OpenAI, this is an LLM primarily focused on conversational interactions.\\n*   **Google Bard:** Similar to ChatGPT, this is another LLM focused on conversational interactions.\\n*   **Llama 2:** An open-source LLM from Meta.\\n*   **GPT-3:** Developed by OpenAI, this is a powerful LLM.\\n*   **Bard:** Developed by Google, another LLM.\\n\\n**Key characteristics:**\\n\\n*   **Large Size:** LLMs are incredibly large, with billions of parameters.\\n*   **Deep Learning:** They are built using deep learning techniques.\\n*   **Neural Networks:** They are based on neural networks.\\n*   **Contextual Understanding:** They have a strong ability to understand the context of text.\\n\\nDo you want me to delve deeper into a specific aspect of LLMs, such as:**\\n\\n*   How they work?\\n*   Their limitations?\\n*   A comparison of different models?\\n*   An example of a specific application?',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 7, 'completion_tokens': 395, 'total_tokens': 402}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKCThmXkFRU",
        "outputId": "d484c4f1-e40a-4286-ba1e-514f38763eb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Large Language Model**\n",
            "\n",
            "A Large Language Model (LLM) is a type of artificial intelligence (AI) that is trained on a massive amount of text data.  These models are designed to understand and generate human-like text.  They can perform a variety of tasks, such as:\n",
            "\n",
            "*   **Answering questions:** You can ask them questions and they will try to provide answers.\n",
            "*   **Generating text:** They can write stories, poems, code, or anything else you request.\n",
            "*   **Summarizing text:** They can quickly condense lengthy articles or documents.\n",
            "*   **Translation:** They can translate text from one language to another.\n",
            "*   **Chatting:** They can engage in conversations.\n",
            "\n",
            "LLMs are a particularly powerful type of AI because of their ability to learn from vast amounts of data.\n",
            "\n",
            "**Examples of LLMs:**\n",
            "\n",
            "*   **ChatGPT:** Developed by OpenAI, this is an LLM primarily focused on conversational interactions.\n",
            "*   **Google Bard:** Similar to ChatGPT, this is another LLM focused on conversational interactions.\n",
            "*   **Llama 2:** An open-source LLM from Meta.\n",
            "*   **GPT-3:** Developed by OpenAI, this is a powerful LLM.\n",
            "*   **Bard:** Developed by Google, another LLM.\n",
            "\n",
            "**Key characteristics:**\n",
            "\n",
            "*   **Large Size:** LLMs are incredibly large, with billions of parameters.\n",
            "*   **Deep Learning:** They are built using deep learning techniques.\n",
            "*   **Neural Networks:** They are based on neural networks.\n",
            "*   **Contextual Understanding:** They have a strong ability to understand the context of text.\n",
            "\n",
            "Do you want me to delve deeper into a specific aspect of LLMs, such as:**\n",
            "\n",
            "*   How they work?\n",
            "*   Their limitations?\n",
            "*   A comparison of different models?\n",
            "*   An example of a specific application?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxOGQzMjknD6"
      },
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}